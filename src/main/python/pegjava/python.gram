# Copyright (c) 2021, Oracle and/or its affiliates.
# Copyright (C) 1996-2021 Python Software Foundation
#
# Licensed under the PYTHON SOFTWARE FOUNDATION LICENSE VERSION 2
#
# PEG grammar for Java

#changes from c gramammar:
#   added java specific meta data @package, @parser_fields,
#   default rule is renamed to default_param

@license '''/*
 * Copyright (c) 2021, Oracle and/or its affiliates. All rights reserved.
 * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
 *
 * The Universal Permissive License (UPL), Version 1.0
 *
 * Subject to the condition set forth below, permission is hereby granted to any
 * person obtaining a copy of this software, associated documentation and/or
 * data (collectively the "Software"), free of charge and under any and all
 * copyright rights in the Software, and any and all patent rights owned or
 * freely licensable by each licensor hereunder covering either (i) the
 * unmodified Software as contributed to or provided by such licensor, or (ii)
 * the Larger Works (as defined below), to deal in both
 *
 * (a) the Software, and
 *
 * (b) any piece of software and/or hardware listed in the lrgrwrks.txt file if
 * one is included with the Software each a "Larger Work" to which the Software
 * is contributed by such licensors),
 *
 * without restriction, including without limitation the rights to copy, create
 * derivative works of, display, perform, and distribute the Software and make,
 * use, sell, offer for sale, import, export, have made, and have sold the
 * Software and the Larger Work(s), and to sublicense the foregoing rights on
 * either these or other terms.
 *
 * This license is subject to the following condition:
 *
 * The above copyright notice and either this complete permission notice or at a
 * minimum a reference to the UPL must be included in all copies or substantial
 * portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 * SOFTWARE.
 */
'''

@package '''
com.oracle.graal.python.pegparser
'''

@imports '''
import com.oracle.graal.python.pegparser.sst.*;
import com.oracle.graal.python.pegparser.tokenizer.Token;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.Set;
import java.util.stream.Collectors;
import java.util.stream.Stream;
'''

@parser_fields '''
    // caching results for pos -> rule
    // the null results are cached as well

    // TODO make a special section for private methods
    private BinaryArithmeticSSTNode finishBinaryOp(SSTNode left, SSTNode[] right) {
        BinaryArithmeticSSTNode rightOp = (BinaryArithmeticSSTNode)right[0];
        return factory.createBinaryOp(rightOp.getOperation(), left, rightOp.getRight(), left.getStartOffset(), rightOp.getEndOffset());
    }
'''

@trailer '''
void *
_PyPegen_parse(Parser *p)
{
    // Initialize keywords
    p->keywords = reserved_keywords;
    p->n_keyword_lists = n_keyword_lists;
    p->soft_keywords = soft_keywords;

    // Run parser
    void *result = NULL;
    if (p->start_rule == Py_file_input) {
        result = file_rule(p);
    } else if (p->start_rule == Py_single_input) {
        result = interactive_rule(p);
    } else if (p->start_rule == Py_eval_input) {
        result = eval_rule(p);
    } else if (p->start_rule == Py_func_type_input) {
        result = func_type_rule(p);
    } else if (p->start_rule == Py_fstring_input) {
        result = fstring_rule(p);
    }

    return result;
}

// The end
'''
file[SSTNode]: a=[statements] ENDMARKER { factory.createModule((SSTNode[])a, startToken.startOffset, endToken.endOffset) } # TODO create module from the statements instead block
interactive[SSTNode]: a=statement_newline { _PyAST_Interactive(a, p->arena) }
eval[SSTNode]: a=expressions NEWLINE* ENDMARKER { a }
func_type[SSTNode]: '(' a=[type_expressions] ')' '->' b=expression NEWLINE* ENDMARKER { _PyAST_FunctionType(a, b, p->arena) }
fstring[SSTNode]: star_expressions

# type_expressions allow */** but ignore them
type_expressions[ExprTy*]:
    | a=','.expression+ ',' '*' b=expression ',' '**' c=expression {
        this.appendToEnd(
            this.appendToEnd(a, b),
            c) }
    | a=','.expression+ ',' '*' b=expression { this.appendToEnd(a, b) }
    | a=','.expression+ ',' '**' b=expression { this.appendToEnd(a, b) }
    | '*' a=expression ',' '**' b=expression {
        this.appendToEnd(
            this.singletonSequence(a),
            b) }
    | '*' a=expression { this.singletonSequence(a) }
    | '**' a=expression { this.singletonSequence(a) }
    | a[SSTNode*]=','.expression+ {a}

statements[StmtTy*]: a=statement+ { a }
statement[StmtTy*]:
    | a=compound_stmt { new StmtTy[]{a} }
    | a[StmtTy*]=simple_stmts { a }
statement_newline[StmtTy*]:
    | a=compound_stmt NEWLINE { this.singletonSequence(a) }
    | simple_stmts
    | NEWLINE { this.singletonSequence(factory.createPass(startToken.startOffset, endToken.endOffset)) }
    | ENDMARKER { _PyPegen_interactive_exit(p) }
simple_stmts[StmtTy*]:
    | a=simple_stmt !';' NEWLINE {
            this.singletonSequence(a); # Not needed, there for speedup
        }
    | a[StmtTy*]=';'.simple_stmt+ [';'] NEWLINE { a }
# NOTE: assignment MUST precede expression, else parsing a simple assignment
# will throw a SyntaxError.
simple_stmt[StmtTy] (memo):
    | assignment
    | e=star_expressions   { e }
    | &'return' return_stmt
    | &('import' | 'from') import_stmt
    | &'raise' raise_stmt
    | 'pass' { factory.createPass(startToken.startOffset, endToken.endOffset) }
    | &'del' del_stmt
    | &'yield' yield_stmt
    | &'assert' assert_stmt
    | 'break' { factory.createBreak(startToken.startOffset, endToken.endOffset) }
    | 'continue' { factory.createContinue(startToken.startOffset, endToken.endOffset) }
    | &'global' global_stmt
    | &'nonlocal' nonlocal_stmt
compound_stmt[StmtTy]:
    | &('def' | '@' | ASYNC) function_def
    | &'if' if_stmt
    | &('class' | '@') class_def
    | &('with' | ASYNC) with_stmt
    | &('for' | ASYNC) for_stmt
    | &'try' try_stmt
    | &'while' while_stmt
    | match_stmt

# NOTE: annotated_rhs may start with 'yield'; yield_expr must start with 'yield'
assignment[StmtTy]:
    | a=NAME ':' b=expression c=['=' d=annotated_rhs { d }] {
        factory.createAnnAssignment(
                    factory.createAnnotation(
                        setExprContext(a, ExprContext.Store),
                        b,
                        a.getStartOffset(), b.getEndOffset()),
                    (SSTNode)c, startToken.startOffset, endToken.endOffset);
        }
    | a=('(' b=single_target ')' { b }
         | single_subscript_attribute_target) ':' b=expression c=['=' d=annotated_rhs { d }] {
        CHECK_VERSION(StmtTy, 6, "Variable annotations syntax is", _PyAST_AnnAssign(a, b, c, 0, EXTRA)) }
    | a[ExprTy*]=(z=star_targets '=' { z })+ b=(yield_expr | star_expressions) !'=' tc=[TYPE_COMMENT] {
         factory.createAssignment(
            a, 
            (SSTNode)b, 
           this.newTypeComment((Token) tc),
             startToken.startOffset, endToken.endOffset) }
    | a=single_target b=augassign ~ c=(yield_expr | star_expressions) {
         factory.createAugAssignment(a, b, (SSTNode)c, startToken.startOffset, endToken.endOffset) }
    | invalid_assignment

augassign[ExprTy___BinOp___Type]:
    | '+=' { ExprTy.BinOp.Operator.ADD }
    | '-=' { ExprTy.BinOp.Operator.SUB }
    | '*=' { ExprTy.BinOp.Operator.MULT }
    | '@=' { ExprTy.BinOp.Operator.MATMULT }
    | '/=' { ExprTy.BinOp.Operator.DIV }
    | '%=' { ExprTy.BinOp.Operator.MOD }
    | '&=' { ExprTy.BinOp.Operator.BITAND }
    | '|=' { ExprTy.BinOp.Operator.BITOR }
    | '^=' { ExprTy.BinOp.Operator.BITXOR }
    | '<<=' { ExprTy.BinOp.Operator.LSHIFT }
    | '>>=' { ExprTy.BinOp.Operator.RSHIFT }
    | '**=' { ExprTy.BinOp.Operator.POW }
    | '//=' { ExprTy.BinOp.Operator.FLOORDIV }

global_stmt[StmtTy]: 'global' a[ExprTy*]=','.NAME+ {
    _PyAST_Global(CHECK(String*, _PyPegen_map_names_to_ids(p, a)), EXTRA) }
nonlocal_stmt[StmtTy]: 'nonlocal' a[ExprTy*]=','.NAME+ {
    _PyAST_Nonlocal(CHECK(String*, _PyPegen_map_names_to_ids(p, a)), EXTRA) }

yield_stmt[StmtTy]: y=yield_expr { y }

assert_stmt[StmtTy]: 'assert' a=expression b=[',' z=expression { z }] { _PyAST_Assert(a, b, EXTRA) }

del_stmt[StmtTy]:
    | 'del' a=del_targets &(';' | NEWLINE) { _PyAST_Delete(a, EXTRA) }
    | invalid_del_stmt

import_stmt[StmtTy]: import_name | import_from
import_name[StmtTy]: 'import' a=dotted_as_names { _PyAST_Import(a, EXTRA) }
# note below: the ('.' | '...') is necessary because '...' is tokenized as ELLIPSIS
import_from[StmtTy]:
    | 'from' a=('.' | '...')* b=dotted_name 'import' c=import_from_targets {
        _PyAST_ImportFrom(b->v.Name.id, c, _PyPegen_seq_count_dots(a), EXTRA) }
    | 'from' a=('.' | '...')+ 'import' b=import_from_targets {
        _PyAST_ImportFrom(NULL, b, _PyPegen_seq_count_dots(a), EXTRA) }
import_from_targets[AliasTy*]:
    | '(' a=import_from_as_names [','] ')' { a }
    | import_from_as_names !','
    | '*' { (AliasTy[])this.singletonSequence(CHECK(alias_ty, _PyPegen_alias_for_star(p, EXTRA))) }
    | invalid_import_from_targets
import_from_as_names[AliasTy*]:
    | a[AliasTy*]=','.import_from_as_name+ { a }
import_from_as_name[alias_ty]:
    | a=NAME b=['as' z=NAME { z }] { _PyAST_alias(a->v.Name.id,
                                               (b) ? ((expr_ty) b)->v.Name.id : NULL,
                                               EXTRA) }
dotted_as_names[AliasTy*]:
    | a[AliasTy*]=','.dotted_as_name+ { a }
dotted_as_name[alias_ty]:
    | a=dotted_name b=['as' z=NAME { z }] { _PyAST_alias(a->v.Name.id,
                                                      (b) ? ((expr_ty) b)->v.Name.id : NULL,
                                                      EXTRA) }
dotted_name[VarLookupSSTNode]:
    | a=dotted_name '.' b=NAME { this.joinNamesWithDot(a, b) }
    | NAME

if_stmt[StmtTy]:
    | 'if' a=named_expression ':' b=block c=elif_stmt {
        _PyAST_If(a, b, CHECK(StmtTy*, this.singletonSequence(c)), EXTRA) }
    | 'if' a=named_expression ':' b=block c=[else_block] { _PyAST_If(a, b, c, EXTRA) }
    | invalid_if_stmt
elif_stmt[StmtTy]:
    | 'elif' a=named_expression ':' b=block c=elif_stmt {
        _PyAST_If(a, b, CHECK(StmtTy*, this.singletonSequence(c)), EXTRA) }
    | 'elif' a=named_expression ':' b=block c=[else_block] { _PyAST_If(a, b, c, EXTRA) }
    | invalid_elif_stmt
else_block[StmtTy*]: 'else' &&':' b=block { b }

while_stmt[StmtTy]:
    | 'while' a=named_expression ':' b=block c=[else_block] { _PyAST_While(a, b, c, EXTRA) }
    | invalid_while_stmt

for_stmt[StmtTy]:
    | 'for' t=star_targets 'in' ~ ex=star_expressions &&':' tc=[TYPE_COMMENT] b=block el=[else_block] {
        _PyAST_For(t, ex, b, el, NEW_TYPE_COMMENT(p, tc), EXTRA) }
    | ASYNC 'for' t=star_targets 'in' ~ ex=star_expressions &&':' tc=[TYPE_COMMENT] b=block el=[else_block] {
        CHECK_VERSION(StmtTy, 5, "Async for loops are", _PyAST_AsyncFor(t, ex, b, el, NEW_TYPE_COMMENT(p, tc), EXTRA)) }
    | invalid_for_target

with_stmt[StmtTy]:
    | 'with' '(' a[asdl_withitem_seq*]=','.with_item+ ','? ')' ':' b=block {
        _PyAST_With(a, b, NULL, EXTRA) }
    | 'with' a[asdl_withitem_seq*]=','.with_item+ ':' tc=[TYPE_COMMENT] b=block {
        _PyAST_With(a, b, NEW_TYPE_COMMENT(p, tc), EXTRA) }
    | ASYNC 'with' '(' a[asdl_withitem_seq*]=','.with_item+ ','? ')' ':' b=block {
       CHECK_VERSION(StmtTy, 5, "Async with statements are", _PyAST_AsyncWith(a, b, NULL, EXTRA)) }
    | ASYNC 'with' a[asdl_withitem_seq*]=','.with_item+ ':' tc=[TYPE_COMMENT] b=block {
       CHECK_VERSION(StmtTy, 5, "Async with statements are", _PyAST_AsyncWith(a, b, NEW_TYPE_COMMENT(p, tc), EXTRA)) }
    | invalid_with_stmt

with_item[withitem_ty]:
    | e=expression 'as' t=star_target &(',' | ')' | ':') { _PyAST_withitem(e, t, p->arena) }
    | invalid_with_item
    | e=expression { _PyAST_withitem(e, NULL, p->arena) }

try_stmt[StmtTy]:
    | 'try' &&':' b=block f=finally_block { _PyAST_Try(b, NULL, NULL, f, EXTRA) }
    | 'try' &&':' b=block ex[asdl_excepthandler_seq*]=except_block+ el=[else_block] f=[finally_block] { _PyAST_Try(b, ex, el, f, EXTRA) }
except_block[excepthandler_ty]:
    | 'except' e=expression t=['as' z=NAME { z }] ':' b=block {
        _PyAST_ExceptHandler(e, (t) ? ((expr_ty) t)->v.Name.id : NULL, b, EXTRA) }
    | 'except' ':' b=block { _PyAST_ExceptHandler(NULL, NULL, b, EXTRA) }
    | invalid_except_block
finally_block[StmtTy*]: 'finally' ':' a=block { a }

match_stmt[StmtTy]:
    | "match" subject=subject_expr ':' NEWLINE INDENT cases[asdl_match_case_seq*]=case_block+ DEDENT {
        CHECK_VERSION(StmtTy, 10, "Pattern matching is", _PyAST_Match(subject, cases, EXTRA)) }
    | invalid_match_stmt
subject_expr[ExprTy]:
    | value=star_named_expression ',' values=star_named_expressions? {
        _PyAST_Tuple(CHECK(asdl_expr_seq*, this.insertInFront(value, values)), Load, EXTRA) }
    | named_expression
case_block[match_case_ty]:
    | "case" pattern=patterns guard=guard? ':' body=block {
        _PyAST_match_case(pattern, guard, body, p->arena) }
    | invalid_case_block
guard[ExprTy]: 'if' guard=named_expression { guard }

patterns[ExprTy]:
    | values[ExprTy*]=open_sequence_pattern {
        _PyAST_Tuple(values, Load, EXTRA) }
    | pattern
pattern[ExprTy]:
    | as_pattern
    | or_pattern
as_pattern[ExprTy]:
    | pattern=or_pattern 'as' target=capture_pattern {
        _PyAST_MatchAs(pattern, target->v.Name.id, EXTRA) }
or_pattern[ExprTy]:
    | patterns[ExprTy*]='|'.closed_pattern+ {
        asdl_seq_LEN(patterns) == 1 ? asdl_seq_GET(patterns, 0) : _PyAST_MatchOr(patterns, EXTRA) }
closed_pattern[ExprTy]:
    | literal_pattern
    | capture_pattern
    | wildcard_pattern
    | value_pattern
    | group_pattern
    | sequence_pattern
    | mapping_pattern
    | class_pattern

literal_pattern[ExprTy]:
    | signed_number !('+' | '-')
    | real=signed_number '+' imag=NUMBER { factory.createBinaryOp(BinaryArithmeticSSTNode.Type.ADD, real, imag, startToken.startOffset, endToken.endOffset ) }
    | real=signed_number '-' imag=NUMBER  { factory.createBinaryOp(BinaryArithmeticSSTNode.Type.SUB, real, imag, startToken.startOffset, endToken.endOffset ) }
    | strings
    | 'None' { factory.createNone(startToken.startOffset, startToken.endOffset) }
    | 'True' { factory.createBooleanLiteral(true, startToken.startOffset, startToken.endOffset) }
    | 'False' { factory.createBooleanLiteral(false, startToken.startOffset, startToken.endOffset) }
signed_number[ExprTy]:
    | NUMBER
    | '-' number=NUMBER { _PyAST_UnaryOp(USub, number, EXTRA) }

capture_pattern[ExprTy]:
    | !"_" name=NAME !('.' | '(' | '=') {
        _PyPegen_set_expr_context(p, name, Store) }

wildcard_pattern[ExprTy]:
    | "_" { _PyAST_Name(CHECK(PyObject*, _PyPegen_new_identifier(p, "_")), Store, EXTRA) }

value_pattern[ExprTy]:
    | attr=attr !('.' | '(' | '=') { attr }
attr[ExprTy]:
    | value=name_or_attr '.' attr=NAME { factory.createGetAttribute(value, ((VarLookupSSTNode)attr).getName(), startToken.startOffset, startToken.endOffset)}
name_or_attr[ExprTy]:
    | attr
    | NAME

group_pattern[ExprTy]:
    | '(' pattern=pattern ')' { pattern }

sequence_pattern[ExprTy]:
    | '[' values=maybe_sequence_pattern? ']' { _PyAST_List(values, Load, EXTRA) }
    | '(' values=open_sequence_pattern? ')' { _PyAST_Tuple(values, Load, EXTRA) }
open_sequence_pattern[asdl_seq*]:
    | value=maybe_star_pattern ',' values=maybe_sequence_pattern? {
        this.insertInFront(value, (SSTNode[])values) }
maybe_sequence_pattern[asdl_seq*]:
    | values=','.maybe_star_pattern+ ','? { values }
maybe_star_pattern[ExprTy]:
    | star_pattern
    | pattern
star_pattern[ExprTy]:
    | '*' value=(capture_pattern | wildcard_pattern) {
        _PyAST_Starred(value, Store, EXTRA) }

mapping_pattern[ExprTy]:
    | '{' items=items_pattern? '}' {
        _PyAST_Dict(CHECK(ExprTy*, _PyPegen_get_keys(p, items)), CHECK(ExprTy*, _PyPegen_get_values(p, items)), EXTRA) }
items_pattern[asdl_seq*]:
    | items=','.key_value_pattern+ ','? { items }
key_value_pattern[AbstractParser___KeyValuePair]:
    | key=(literal_pattern | value_pattern) ':' value=pattern {
        AbstractParser.KeyValuePair(key, value) }
    | double_star_pattern
double_star_pattern[AbstractParser___KeyValuePair]:
    | '**' value=capture_pattern { AbstractParser.KeyValuePair(null, value) }

class_pattern[ExprTy]:
    | func=name_or_attr '(' ')' { _PyAST_Call(func, NULL, NULL, EXTRA) }
    | func=name_or_attr '(' args=positional_patterns ','? ')' {
        _PyAST_Call(func, args, NULL, EXTRA) }
    | func=name_or_attr '(' keywords=keyword_patterns ','? ')' {
        _PyAST_Call(func, NULL, keywords, EXTRA) }
    | func=name_or_attr '(' args=positional_patterns ',' keywords=keyword_patterns ','? ')' {
        _PyAST_Call(func, args, keywords, EXTRA) }
positional_patterns[ExprTy*]:
    | args[ExprTy*]=','.pattern+ { args }
keyword_patterns[asdl_keyword_seq*]:
    | keywords[asdl_keyword_seq*]=','.keyword_pattern+ { keywords }
keyword_pattern[keyword_ty]:
    | arg=NAME '=' value=pattern { _PyAST_keyword(arg->v.Name.id, value, EXTRA) }

return_stmt[StmtTy]:
    | 'return' a=[star_expressions] { _PyAST_Return(a, EXTRA) }

raise_stmt[StmtTy]:
    | 'raise' a=expression b=['from' z=expression { z }] { _PyAST_Raise(a, b, EXTRA) }
    | 'raise' { _PyAST_Raise(NULL, NULL, EXTRA) }

function_def[StmtTy]:
    | d=decorators f=function_def_raw { _PyPegen_function_def_decorators(p, d, f) }
    | function_def_raw

function_def_raw[StmtTy]:
    | 'def' n=NAME '(' params=[params] ')' a=['->' z=expression { z }] &&':' tc=[func_type_comment] b=block {
        factory.createFunctionDef(((VarLookupSSTNode)n).getName(),
                        (ArgumentsSSTNode)params,
                        b, null, (SSTNode)a, this.newTypeComment((Token)tc),
                        startToken.startOffset, endToken.endOffset) }
    | ASYNC 'def' n=NAME '(' params=[params] ')' a=['->' z=expression { z }] &&':' tc=[func_type_comment] b=block {
        CHECK_VERSION(
            StmtTy,
            5,
            "Async functions are",
            _PyAST_AsyncFunctionDef(n->v.Name.id,
                            (params) ? params : CHECK(arguments_ty, _PyPegen_empty_arguments(p)),
                            b, NULL, a, NEW_TYPE_COMMENT(p, tc), EXTRA)
        ) }
func_type_comment[Token]:
    | NEWLINE t=TYPE_COMMENT &(NEWLINE INDENT) { t }  # Must be followed by indented block
    | invalid_double_type_comments
    | TYPE_COMMENT

params[ArgDefListBuilder]:
    | invalid_parameters
    | parameters

parameters[ArgDefListBuilder]:
    | a=slash_no_default b[asdl_arg_seq*]=param_no_default* c=param_with_default* d=[star_etc] {
        _PyPegen_make_arguments(p, a, NULL, b, c, d) }
    | a=slash_with_default b=param_with_default* c=[star_etc] {
        _PyPegen_make_arguments(p, NULL, a, NULL, b, c) }
    | a[asdl_arg_seq*]=param_no_default+ b=param_with_default* c=[star_etc] {
        _PyPegen_make_arguments(p, NULL, NULL, a, b, c) }
    | a=param_with_default+ b=[star_etc] { _PyPegen_make_arguments(p, NULL, NULL, NULL, a, b)}
    | a=star_etc { _PyPegen_make_arguments(p, NULL, NULL, NULL, NULL, a) }

# Some duplication here because we can't write (',' | &')'),
# which is because we don't support empty alternatives (yet).
#
slash_no_default[asdl_arg_seq*]:
    | a[asdl_arg_seq*]=param_no_default+ '/' ',' { a }
    | a[asdl_arg_seq*]=param_no_default+ '/' &')' { a }
slash_with_default[SlashWithDefault*]:
    | a=param_no_default* b=param_with_default+ '/' ',' { _PyPegen_slash_with_default(p, (asdl_arg_seq *)a, b) }
    | a=param_no_default* b=param_with_default+ '/' &')' { _PyPegen_slash_with_default(p, (asdl_arg_seq *)a, b) }

star_etc[StarEtc*]:
    | '*' a=param_no_default b=param_maybe_default* c=[kwds] {
        _PyPegen_star_etc(p, a, b, c) }
    | '*' ',' b=param_maybe_default+ c=[kwds] {
        _PyPegen_star_etc(p, NULL, b, c) }
    | a=kwds { _PyPegen_star_etc(p, NULL, NULL, a) }
    | invalid_star_etc

kwds[arg_ty]: '**' a=param_no_default { a }

# One parameter.  This *includes* a following comma and type comment.
#
# There are three styles:
# - No default
# - With default
# - Maybe with default
#
# There are two alternative forms of each, to deal with type comments:
# - Ends in a comma followed by an optional type comment
# - No comma, optional type comment, must be followed by close paren
# The latter form is for a final parameter without trailing comma.
#
param_no_default[AnnotationSSTNode]:
    | a=param ',' tc=TYPE_COMMENT? { factory.createAnnotation(a.getLhs(), a.getType(), newTypeComment(tc), a.getStartOffset(), a.getEndOffset()) }
    | a=param tc=TYPE_COMMENT? &')' { factory.createAnnotation(a.getLhs(), a.getType(), newTypeComment(tc), a.getStartOffset(), a.getEndOffset()) }
param_with_default[AnnotationSSTNode]:
    | a=param c=default_param ',' tc=TYPE_COMMENT? { factory.createAnnotation(factory.createKeyValuePair(a, c), null, getText((Token)tc), a.getStartOffset(), c.getEndOffset()) }
    | a=param c=default_param tc=TYPE_COMMENT? &')' { factory.createAnnotation(factory.createKeyValuePair(a, c), null, getText((Token)tc), a.getStartOffset(), c.getEndOffset()) }
param_maybe_default[AnnotationSSTNode]:
    | a=param c=default_param? ',' tc=TYPE_COMMENT? { factory.createAnnotation(factory.createKeyValuePair(a, (SSTNode)c), null, getText((Token)tc), a.getStartOffset(), c == null ? a.getEndOffset() : ((SSTNode)c).getEndOffset()) }
    | a=param c=default_param? tc=TYPE_COMMENT? &')' { factory.createAnnotation(factory.createKeyValuePair(a, (SSTNode)c), null, getText((Token)tc), a.getStartOffset(), c == null ? a.getEndOffset() : ((SSTNode)c).getEndOffset()) }
param[AnnotationSSTNode]: a=NAME b=annotation? { factory.createAnnotation(a, (SSTNode)b, startToken.startOffset, endToken.endOffset) }

annotation[ExprTy]: ':' a=expression { a }
default_param[ExprTy]: '=' a=expression { a }

decorators[ExprTy*]: a[ExprTy*]=('@' f=named_expression NEWLINE { f })+ { a }

class_def[StmtTy]:
    | a=decorators b=class_def_raw { _PyPegen_class_def_decorators(p, a, b) }
    | class_def_raw
class_def_raw[StmtTy]:
    | 'class' a=NAME b=['(' z=[arguments] ')' { z }] &&':' c=block {
        _PyAST_ClassDef(a->v.Name.id,
                     (b) ? ((expr_ty) b)->v.Call.args : NULL,
                     (b) ? ((expr_ty) b)->v.Call.keywords : NULL,
                     c, NULL, EXTRA) }

block[StmtTy*] (memo):
    | NEWLINE INDENT a=statements DEDENT { a }
    | simple_stmts
    | invalid_block

star_expressions[ExprTy]:
    | a=star_expression b=(',' c=star_expression { c })+ [','] {
        _PyAST_Tuple(CHECK(ExprTy*, this.insertInFront(a, b)), Load, EXTRA) }
    | a=star_expression ',' { _PyAST_Tuple(CHECK(ExprTy*, this.singletonSequence(a)), Load, EXTRA) }
    | star_expression
star_expression[ExprTy] (memo):
    | '*' a=bitwise_or { _PyAST_Starred(a, Load, EXTRA) }
    | expression

star_named_expressions[ExprTy*]: a[ExprTy*]=','.star_named_expression+ [','] { a }
star_named_expression[ExprTy]:
    | '*' a=bitwise_or { _PyAST_Starred(a, Load, EXTRA) }
    | named_expression

named_expression[ExprTy]:
    | a=NAME ':=' ~ b=expression { _PyAST_NamedExpr(CHECK(expr_ty, _PyPegen_set_expr_context(p, a, Store)), b, EXTRA) }
    | invalid_named_expression
    | expression !':='

direct_named_expression[ExprTy]:
    | a=NAME ':=' ~ b=expression { _PyAST_NamedExpr(CHECK(expr_ty, _PyPegen_set_expr_context(p, a, Store)), b, EXTRA) }
    | expression !':='

annotated_rhs[ExprTy]: yield_expr | star_expressions

expressions[ExprTy]:
    | a=expression b=(',' c=expression { c })+ [','] {
        _PyAST_Tuple(CHECK(ExprTy*, this.insertInFront(a, b)), Load, EXTRA) }
    | a=expression ',' { _PyAST_Tuple(CHECK(ExprTy*, this.singletonSequence(a)), Load, EXTRA) }
    | expression
expression[ExprTy] (memo):
    | invalid_expression
    | a=disjunction 'if' b=disjunction 'else' c=expression { _PyAST_IfExp(b, a, c, EXTRA) }
    | disjunction
    | lambdef

lambdef[ExprTy]:
    | 'lambda' a=[lambda_params] ':' b=expression {
        _PyAST_Lambda((a) ? a : CHECK(arguments_ty, _PyPegen_empty_arguments(p)), b, EXTRA) }

lambda_params[arguments_ty]:
    | invalid_lambda_parameters
    | lambda_parameters

# lambda_parameters etc. duplicates parameters but without annotations
# or type comments, and if there's no comma after a parameter, we expect
# a colon, not a close parenthesis.  (For more, see parameters above.)
#
lambda_parameters[arguments_ty]:
    | a=lambda_slash_no_default b[asdl_arg_seq*]=lambda_param_no_default* c=lambda_param_with_default* d=[lambda_star_etc] {
        _PyPegen_make_arguments(p, a, NULL, b, c, d) }
    | a=lambda_slash_with_default b=lambda_param_with_default* c=[lambda_star_etc] {
        _PyPegen_make_arguments(p, NULL, a, NULL, b, c) }
    | a[asdl_arg_seq*]=lambda_param_no_default+ b=lambda_param_with_default* c=[lambda_star_etc] {
        _PyPegen_make_arguments(p, NULL, NULL, a, b, c) }
    | a=lambda_param_with_default+ b=[lambda_star_etc] { _PyPegen_make_arguments(p, NULL, NULL, NULL, a, b)}
    | a=lambda_star_etc { _PyPegen_make_arguments(p, NULL, NULL, NULL, NULL, a) }

lambda_slash_no_default[asdl_arg_seq*]:
    | a[asdl_arg_seq*]=lambda_param_no_default+ '/' ',' { a }
    | a[asdl_arg_seq*]=lambda_param_no_default+ '/' &':' { a }
lambda_slash_with_default[SlashWithDefault*]:
    | a=lambda_param_no_default* b=lambda_param_with_default+ '/' ',' { _PyPegen_slash_with_default(p, (asdl_arg_seq *)a, b) }
    | a=lambda_param_no_default* b=lambda_param_with_default+ '/' &':' { _PyPegen_slash_with_default(p, (asdl_arg_seq *)a, b) }

lambda_star_etc[StarEtc*]:
    | '*' a=lambda_param_no_default b=lambda_param_maybe_default* c=[lambda_kwds] {
        _PyPegen_star_etc(p, a, b, c) }
    | '*' ',' b=lambda_param_maybe_default+ c=[lambda_kwds] {
        _PyPegen_star_etc(p, NULL, b, c) }
    | a=lambda_kwds { _PyPegen_star_etc(p, NULL, NULL, a) }
    | invalid_lambda_star_etc

lambda_kwds[arg_ty]: '**' a=lambda_param_no_default { a }

lambda_param_no_default[arg_ty]:
    | a=lambda_param ',' { a }
    | a=lambda_param &':' { a }
lambda_param_with_default[NameDefaultPair*]:
    | a=lambda_param c=default_param ',' { _PyPegen_name_default_pair(p, a, c, NULL) }
    | a=lambda_param c=default_param &':' { _PyPegen_name_default_pair(p, a, c, NULL) }
lambda_param_maybe_default[NameDefaultPair*]:
    | a=lambda_param c=default_param? ',' { _PyPegen_name_default_pair(p, a, c, NULL) }
    | a=lambda_param c=default_param? &':' { _PyPegen_name_default_pair(p, a, c, NULL) }
lambda_param[arg_ty]: a=NAME { _PyAST_arg(a->v.Name.id, NULL, NULL, EXTRA) }

disjunction[ExprTy] (memo):
    | a=conjunction b=('or' c=conjunction { c })+ { _PyAST_BoolOp(
        Or,
        CHECK(ExprTy*, this.insertInFront(a, b)),
        EXTRA) }
    | conjunction
conjunction[ExprTy] (memo):
    | a=inversion b=('and' c=inversion { c })+ { _PyAST_BoolOp(
        And,
        CHECK(ExprTy*, this.insertInFront(a, b)),
        EXTRA) }
    | inversion
inversion[ExprTy] (memo):
    | 'not' a=inversion { _PyAST_UnaryOp(Not, a, EXTRA) }
    | comparison
comparison[ExprTy]:
    | a=bitwise_or b=compare_op_bitwise_or_pair+ { finishBinaryOp(a, b)}
    | bitwise_or
compare_op_bitwise_or_pair[BinaryArithmeticSSTNode]:
    | eq_bitwise_or
    | noteq_bitwise_or
    | lte_bitwise_or
    | lt_bitwise_or
    | gte_bitwise_or
    | gt_bitwise_or
    | notin_bitwise_or
    | in_bitwise_or
    | isnot_bitwise_or
    | is_bitwise_or
eq_bitwise_or[BinaryArithmeticSSTNode]: '==' a=bitwise_or { factory.createBinaryOp(BinaryArithmeticSSTNode.Type.EQ, null, a, startToken.startOffset, endToken.endOffset)}#{ _PyPegen_cmpop_expr_pair(p, Eq, a) }
noteq_bitwise_or[CmpopExprPair*]:
    | (tok='!=' { _PyPegen_check_barry_as_flufl(p, tok) ? NULL : tok}) a=bitwise_or {_PyPegen_cmpop_expr_pair(p, NotEq, a) }
lte_bitwise_or[CmpopExprPair*]: '<=' a=bitwise_or { _PyPegen_cmpop_expr_pair(p, LtE, a) }
lt_bitwise_or[CmpopExprPair*]: '<' a=bitwise_or { _PyPegen_cmpop_expr_pair(p, Lt, a) }
gte_bitwise_or[CmpopExprPair*]: '>=' a=bitwise_or { _PyPegen_cmpop_expr_pair(p, GtE, a) }
gt_bitwise_or[CmpopExprPair*]: '>' a=bitwise_or { _PyPegen_cmpop_expr_pair(p, Gt, a) }
notin_bitwise_or[CmpopExprPair*]: 'not' 'in' a=bitwise_or { _PyPegen_cmpop_expr_pair(p, NotIn, a) }
in_bitwise_or[CmpopExprPair*]: 'in' a=bitwise_or { _PyPegen_cmpop_expr_pair(p, In, a) }
isnot_bitwise_or[CmpopExprPair*]: 'is' 'not' a=bitwise_or { _PyPegen_cmpop_expr_pair(p, IsNot, a) }
is_bitwise_or[CmpopExprPair*]: 'is' a=bitwise_or { _PyPegen_cmpop_expr_pair(p, Is, a) }

bitwise_or[ExprTy]:
    | a=bitwise_or '|' b=bitwise_xor { factory.createBinaryOp(BinaryArithmeticSSTNode.Type.BIT_OR, a, b, startToken.startOffset, endToken.endOffset ) }
    | bitwise_xor
bitwise_xor[ExprTy]:
    | a=bitwise_xor '^' b=bitwise_and { factory.createBinaryOp(BinaryArithmeticSSTNode.Type.BIT_XOR, a, b, startToken.startOffset, endToken.endOffset ) }
    | bitwise_and
bitwise_and[ExprTy]:
    | a=bitwise_and '&' b=shift_expr { factory.createBinaryOp(BinaryArithmeticSSTNode.Type.BIT_AND, a, b, startToken.startOffset, endToken.endOffset ) }
    | shift_expr
shift_expr[ExprTy]:
    | a=shift_expr '<<' b=sum { factory.createBinaryOp(BinaryArithmeticSSTNode.Type.LSHIFT, a, b, startToken.startOffset, endToken.endOffset ) }
    | a=shift_expr '>>' b=sum { factory.createBinaryOp(BinaryArithmeticSSTNode.Type.RSHIFT, a, b, startToken.startOffset, endToken.endOffset ) }
    | sum

sum[ExprTy]:
    | a=sum '+' b=term { factory.createBinaryOp(BinaryArithmeticSSTNode.Type.ADD, a, b, startToken.startOffset, endToken.endOffset ) }
    | a=sum '-' b=term { factory.createBinaryOp(BinaryArithmeticSSTNode.Type.SUB, a, b, startToken.startOffset, endToken.endOffset ) }
    | term
term[ExprTy]:
    | a=term '*' b=factor { factory.createBinaryOp(BinaryArithmeticSSTNode.Type.MULT, a, b, startToken.startOffset, endToken.endOffset ) }
    | a=term '/' b=factor { factory.createBinaryOp(BinaryArithmeticSSTNode.Type.DIV, a, b, startToken.startOffset, endToken.endOffset ) }
    | a=term '//' b=factor { factory.createBinaryOp(BinaryArithmeticSSTNode.Type.FLOOR_DIV, a, b, startToken.startOffset, endToken.endOffset ) }
    | a=term '%' b=factor { factory.createBinaryOp(BinaryArithmeticSSTNode.Type.MOD, a, b, startToken.startOffset, endToken.endOffset ) }
    | a=term '@' b=factor { CHECK_VERSION(expr_ty, 5, "The '@' operator is", _PyAST_BinOp(a, MatMult, b, EXTRA)) }
    | factor
factor[ExprTy] (memo):
    | '+' a=factor {factory.createUnaryOp(UnarySSTNode.Type.ADD, a, startToken.startOffset, endToken.endOffset)} #{ _PyAST_UnaryOp(UAdd, a, EXTRA) }
    | '-' a=factor {factory.createUnaryOp(UnarySSTNode.Type.SUB, a, startToken.startOffset, endToken.endOffset)} #{ _PyAST_UnaryOp(USub, a, EXTRA) }
    | '~' a=factor {factory.createUnaryOp(UnarySSTNode.Type.INVERT, a, startToken.startOffset, endToken.endOffset)} #{ _PyAST_UnaryOp(Invert, a, EXTRA) }
    | power
power[ExprTy]:
    | a=await_primary '**' b=factor { factory.createBinaryOp(BinaryArithmeticSSTNode.Type.POW, a, b, startToken.startOffset, endToken.endOffset ) }
    | await_primary
await_primary[ExprTy] (memo):
    | AWAIT a=primary { CHECK_VERSION(expr_ty, 5, "Await expressions are", _PyAST_Await(a, EXTRA)) }
    | primary
primary[ExprTy]:
    | invalid_primary  # must be before 'primay genexp' because of invalid_genexp
    | a=primary '.' b=NAME { factory.createGetAttribute(a, ((VarLookupSSTNode)b).getName(), startToken.startOffset, startToken.endOffset) }
    | a=primary b=genexp { _PyAST_Call(a, CHECK(ExprTy*, (ExprTy*)this.singletonSequence(b)), NULL, EXTRA) }
    | a=primary '(' b[SSTNode*]=[arguments] ')' {
        factory.createCall(a, this.extractArgs(b), this.extractKeywordArgs(b), startToken.startOffset, endToken.endOffset ) }
    | a=primary '[' b=slices ']' { factory.createSubscript(a, b, startToken.startOffset, startToken.endOffset) }
    | atom

slices[ExprTy]:
    | a=slice !',' { a }
    | a[ExprTy*]=','.slice+ [','] { _PyAST_Tuple(a, Load, EXTRA) }
slice[ExprTy]:
    | a=[expression] ':' b=[expression] c=[':' d=[expression] { d }] { _PyAST_Slice(a, b, c, EXTRA) }
    | a=named_expression { a }
atom[ExprTy]:
    | NAME
    | 'True' { factory.createBooleanLiteral(true, startToken.startOffset, endToken.endOffset) }
    | 'False' { factory.createBooleanLiteral(false, startToken.startOffset, endToken.endOffset) }
    | 'None' { factory.createNone(startToken.startOffset, startToken.endOffset) }
    | &STRING strings
    | NUMBER
    | &'(' (tuple | group | genexp)
    | &'[' (list | listcomp)
    | &'{' (dict | set | dictcomp | setcomp)
    | '...' { factory.createEllipsis(startToken.startOffset, startToken.endOffset) }

strings[SSTNode] (memo): a=STRING+ { this.concatenateStrings(a) }
list[ExprTy]:
    | '[' a=[star_named_expressions] ']' { factory.createList((SSTNode[])a, startToken.startOffset, endToken.endOffset) }
listcomp[ExprTy]:
    | '[' a=named_expression b=for_if_clauses ']' { factory.createListComprehension(a, b, startToken.startOffset, endToken.endOffset) }
    | invalid_comprehension
tuple[ExprTy]:
    | '(' a=[y=star_named_expression ',' z=[star_named_expressions] { this.insertInFront(y, (SSTNode[])z) } ] ')' {
        factory.createTuple((SSTNode[])a, startToken.startOffset, endToken.endOffset) }
group[ExprTy]:
    | '(' a=(yield_expr | named_expression) ')' { a }
    | invalid_group
genexp[ExprTy]:
    | '(' a=direct_named_expression b=for_if_clauses ')' { factory.createGenerator(a, b, startToken.startOffset, endToken.endOffset) }
    | invalid_comprehension
set[SSTNode]: '{' a=star_named_expressions '}' { factory.createSet((SSTNode[])a,startToken.startOffset,endToken.endOffset) }
setcomp[ExprTy]:
    | '{' a=named_expression b=for_if_clauses '}' { factory.createSetComprehension(a, b, startToken.startOffset, endToken.endOffset) }
    | invalid_comprehension
dict[ExprTy]:
    | '{' a=[double_starred_kvpairs] '}' {
        factory.createDict((SSTNode[])a,startToken.startOffset,endToken.endOffset) }
    | '{' invalid_double_starred_kvpairs '}'

dictcomp[ExprTy]:
    | '{' a=kvpair b=for_if_clauses '}' { factory.createDictComprehension(a, b, startToken.startOffset, endToken.endOffset) }
    | invalid_dict_comprehension
double_starred_kvpairs[AbstractParser___KeyValuePair]: a=','.double_starred_kvpair+ [','] { a }
double_starred_kvpair[AbstractParser___KeyValuePair]:
    | '**' a=bitwise_or { AbstractParser.KeyValuePair(null, a) }
    | kvpair
kvpair[AbstractParser___KeyValuePair]: a=expression ':' b=expression { AbstractParser.KeyValuePair(a, b) }
for_if_clauses[ForComprehensionSSTNode*]:
    | a[ForComprehensionSSTNode*]=for_if_clause+ { a }
for_if_clause[ForComprehensionSSTNode]:
    | ASYNC 'for' a=star_targets 'in' ~ b=disjunction c[ExprTy*]=('if' z=disjunction { z })* {
        CHECK_VERSION(comprehension_ty, 6, "Async comprehensions are", _PyAST_comprehension(a, b, c, 1, p->arena)) }
    | 'for' a=star_targets 'in' ~ b=disjunction c[SSTNode*]=('if' z=disjunction { z })* {
        factory.createComprehension(a, b, c, false, startToken.startOffset, endToken.endOffset) }
    | invalid_for_target

yield_expr[ExprTy]:
    | 'yield' 'from' a=expression { factory.createYield((SSTNode)a, true, startToken.startOffset, endToken.endOffset) }
    | 'yield' a=[star_expressions] { factory.createYield((SSTNode)a, false, startToken.startOffset, endToken.endOffset) }

arguments[ExprTy*] (memo):
    | a=args [','] &')' { a }
    | invalid_arguments
args[ExprTy*]:
    | a=','.(starred_expression | direct_named_expression !'=')+ b[SSTNode*]=[',' k=kwargs {k}] { this.join(a, b) }
    | a=kwargs { a }
kwargs[ExprTy*]:
    | a=','.kwarg_or_starred+ ',' b[SSTNode*]=','.kwarg_or_double_starred+ { this.join(a, b) }
    | ','.kwarg_or_starred+
    | ','.kwarg_or_double_starred+
starred_expression[ExprTy]:
    | '*' a=expression { factory.createStarred(a, startToken.startOffset, endToken.endOffset) }
kwarg_or_starred[AbstractParser___KeywordOrStarred]:
    | a=NAME '=' b=expression { AbstractParser.KeywordOrStarred(factory.createKeyword(a, b, startToken.startOffset, endToken.endOffset), true) }
    | a=starred_expression { a }
    | invalid_kwarg
kwarg_or_double_starred[AbstractParser___KeywordOrStarred]:
    | a=NAME '=' b=expression { AbstractParser.KeywordOrStarred(factory.createKeyword(a, b, startToken.startOffset, endToken.endOffset), true) }
    | '**' a=expression { AbstractParser.KeywordOrStarred(factory.createKeyword(NULL, a, startToken.startOffset, endToken.endOffset), true) }
    | invalid_kwarg

# NOTE: star_targets may contain *bitwise_or, targets may not.
star_targets[ExprTy]:
    | a=star_target !',' { a }
    | a=star_target b=(',' c=star_target { c })* [','] {
        factory.createTuple(this.insertInFront(a,b),startToken.startOffset,endToken.endOffset); // Store }
star_targets_list_seq[ExprTy*]: a[ExprTy*]=','.star_target+ [','] { a }
star_targets_tuple_seq[ExprTy*]:
    | a=star_target b=(',' c=star_target { c })+ [','] { (ExprTy*) this.insertInFront(a, b) }
    | a=star_target ',' { (ExprTy*) this.singletonSequence(a) }
star_target[ExprTy] (memo):
    | '*' a=(!'*' star_target) {
        _PyAST_Starred(CHECK(expr_ty, _PyPegen_set_expr_context(p, a, Store)), Store, EXTRA) }
    | target_with_star_atom
target_with_star_atom[ExprTy] (memo):
    | a=t_primary '.' b=NAME !t_lookahead { factory.createGetAttribute(a, ((VarLookupSSTNode)b).getName(), startToken.startOffset, startToken.endOffset) }
    | a=t_primary '[' b=slices ']' !t_lookahead { factory.createSubscript(a, b, startToken.startOffset, startToken.endOffset) }
    | star_atom
star_atom[ExprTy]:
    | a=NAME { this.setExprContext(a, ExprContext.Store) }
    | '(' a=target_with_star_atom ')' { this.setExprContext(a, ExprContext.Store) }
    | '(' a=[star_targets_tuple_seq] ')' { factory.createTuple((SSTNode[])a, startToken.startOffset, endToken.endOffset) }
    | '[' a=[star_targets_list_seq] ']' { factory.createList((SSTNode[])a, startToken.startOffset, endToken.endOffset) }

single_target[ExprTy]:
    | single_subscript_attribute_target
    | a=NAME  { this.setExprContext(a, ExprContext.Store) }
    | '(' a=single_target ')' { a }
single_subscript_attribute_target[ExprTy]:
    | a=t_primary '.' b=NAME !t_lookahead { factory.createGetAttribute(a, ((VarLookupSSTNode)b).getName(), startToken.startOffset, startToken.endOffset) }
    | a=t_primary '[' b=slices ']' !t_lookahead { factory.createSubscript(a, b, startToken.startOffset, startToken.endOffset) }

del_targets[ExprTy*]: a[ExprTy*]=','.del_target+ [','] { a }
del_target[ExprTy] (memo):
    | a=t_primary '.' b=NAME !t_lookahead { factory.createGetAttribute(a, ((VarLookupSSTNode)b).getName(), startToken.startOffset, startToken.endOffset) }
    | a=t_primary '[' b=slices ']' !t_lookahead { factory.createSubscript(a, b, startToken.startOffset, startToken.endOffset) }
    | del_t_atom
del_t_atom[ExprTy]:
    | a=NAME { _PyPegen_set_expr_context(p, a, Del) }
    | '(' a=del_target ')' { _PyPegen_set_expr_context(p, a, Del) }
    | '(' a=[del_targets] ')' { _PyAST_Tuple(a, Del, EXTRA) }
    | '[' a=[del_targets] ']' { _PyAST_List(a, Del, EXTRA) }

targets[ExprTy*]: a[ExprTy*]=','.target+ [','] { a }
target[ExprTy] (memo):
    | a=t_primary '.' b=NAME !t_lookahead { factory.createGetAttribute(a, ((VarLookupSSTNode)b).getName(), startToken.startOffset, startToken.endOffset) }
    | a=t_primary '[' b=slices ']' !t_lookahead { factory.createSubscript(a, b, startToken.startOffset, startToken.endOffset) }
    | t_atom
t_primary[ExprTy]:
    | a=t_primary '.' b=NAME &t_lookahead { factory.createGetAttribute(a, ((VarLookupSSTNode)b).getName(), startToken.startOffset, startToken.endOffset) }
    | a=t_primary '[' b=slices ']' &t_lookahead { factory.createSubscript(a, b, startToken.startOffset, startToken.endOffset) }
    | a=t_primary b=genexp &t_lookahead {
        _PyAST_Call(a, CHECK(ExprTy*, (ExprTy*)this.singletonSequence(b)), NULL, EXTRA) }
    | a=t_primary '(' b[SSTNode*]=[arguments] ')' &t_lookahead {
            factory.createCall(a, this.extractArgs(b), this.extractKeywordArgs(b), startToken.startOffset, endToken.endOffset ) 
        }
    | a=atom &t_lookahead { a }
t_lookahead: '(' | '[' | '.'
t_atom[ExprTy]:
    | a=NAME { _PyPegen_set_expr_context(p, a, Store) }
    | '(' a=target ')' { _PyPegen_set_expr_context(p, a, Store) }
    | '(' b=[targets] ')' { _PyAST_Tuple(b, Store, EXTRA) }
    | '[' b=[targets] ']' { _PyAST_List(b, Store, EXTRA) }


# From here on, there are rules for invalid syntax with specialised error messages
invalid_arguments:
    | args ',' '*' { RAISE_SYNTAX_ERROR("iterable argument unpacking follows keyword argument unpacking") }
    | a=expression for_if_clauses ',' [args | expression for_if_clauses] {
        RAISE_SYNTAX_ERROR_KNOWN_LOCATION(a, "Generator expression must be parenthesized") }
    | a=args for_if_clauses { _PyPegen_nonparen_genexp_in_call(p, a) }
    | args ',' a=expression for_if_clauses {
        RAISE_SYNTAX_ERROR_KNOWN_LOCATION(a, "Generator expression must be parenthesized") }
    | a=args ',' args { _PyPegen_arguments_parsing_error(p, a) }
invalid_kwarg:
    | expression a='=' {
        RAISE_SYNTAX_ERROR_KNOWN_LOCATION(
            a, "expression cannot contain assignment, perhaps you meant \"==\"?") }

invalid_expression:
    # !(NAME STRING) is not matched so we don't show this error with some invalid string prefixes like: kf"dsfsdf"
    # Soft keywords need to also be ignored because they can be parsed as NAME NAME
    | !(NAME STRING | SOFT_KEYWORD) a=disjunction expression {
        RAISE_ERROR_KNOWN_LOCATION(p, PyExc_SyntaxError, a->lineno, a->end_col_offset - 1, "invalid syntax. Perhaps you forgot a comma?") }

invalid_named_expression:
    | a=expression ':=' expression {
        RAISE_SYNTAX_ERROR_KNOWN_LOCATION(
            a, "cannot use assignment expressions with %s", _PyPegen_get_expr_name(a)) }
    | a=NAME b='=' bitwise_or !('='|':='|',') {
        RAISE_SYNTAX_ERROR_KNOWN_LOCATION(b, "invalid syntax. Maybe you meant '==' or ':=' instead of '='?") }
    | !(list|tuple|genexp|'True'|'None'|'False') a=bitwise_or b='=' bitwise_or !('='|':='|',') {
        RAISE_SYNTAX_ERROR_KNOWN_LOCATION(b, "cannot assign to %s here. Maybe you meant '==' instead of '='?",
                                          _PyPegen_get_expr_name(a)) }

invalid_assignment:
    | a=invalid_ann_assign_target ':' expression {
        RAISE_SYNTAX_ERROR_KNOWN_LOCATION(
            a,
            "only single target (not %s) can be annotated",
            _PyPegen_get_expr_name(a)
        )}
    | a=star_named_expression ',' star_named_expressions* ':' expression {
        RAISE_SYNTAX_ERROR_KNOWN_LOCATION(a, "only single target (not tuple) can be annotated") }
    | a=expression ':' expression {
        RAISE_SYNTAX_ERROR_KNOWN_LOCATION(a, "illegal target for annotation") }
    | (star_targets '=')* a=star_expressions '=' {
        RAISE_SYNTAX_ERROR_INVALID_TARGET(STAR_TARGETS, a) }
    | (star_targets '=')* a=yield_expr '=' { RAISE_SYNTAX_ERROR_KNOWN_LOCATION(a, "assignment to yield expression not possible") }
    | a=star_expressions augassign (yield_expr | star_expressions) {
        RAISE_SYNTAX_ERROR_KNOWN_LOCATION(
            a,
            "'%s' is an illegal expression for augmented assignment",
            _PyPegen_get_expr_name(a)
        )}
invalid_ann_assign_target[ExprTy]:
    | list
    | tuple
    | '(' a=invalid_ann_assign_target ')' { a }
invalid_del_stmt:
    | 'del' a=star_expressions {
        RAISE_SYNTAX_ERROR_INVALID_TARGET(DEL_TARGETS, a) }
invalid_block:
    | NEWLINE !INDENT { RAISE_INDENTATION_ERROR("expected an indented block") }
invalid_primary:
    | primary a='{' { RAISE_SYNTAX_ERROR_KNOWN_LOCATION(a, "invalid syntax") }
invalid_comprehension:
    | ('[' | '(' | '{') a=starred_expression for_if_clauses {
        RAISE_SYNTAX_ERROR_KNOWN_LOCATION(a, "iterable unpacking cannot be used in comprehension") }
    | ('[' | '{') a=star_named_expression ',' [star_named_expressions] for_if_clauses {
        RAISE_SYNTAX_ERROR_KNOWN_LOCATION(a, "did you forget parentheses around the comprehension target?") }
invalid_dict_comprehension:
    | '{' a='**' bitwise_or for_if_clauses '}' {
        RAISE_SYNTAX_ERROR_KNOWN_LOCATION(a, "dict unpacking cannot be used in dict comprehension") }
invalid_parameters:
    | param_no_default* invalid_parameters_helper param_no_default {
        RAISE_SYNTAX_ERROR("non-default argument follows default argument") }
invalid_parameters_helper: # This is only there to avoid type errors
    | a=slash_with_default { this.singletonSequence(a) }
    | param_with_default+
invalid_lambda_parameters:
    | lambda_param_no_default* invalid_lambda_parameters_helper lambda_param_no_default {
        RAISE_SYNTAX_ERROR("non-default argument follows default argument") }
invalid_lambda_parameters_helper:
    | a=lambda_slash_with_default { this.singletonSequence(a) }
    | lambda_param_with_default+
invalid_star_etc:
    | '*' (')' | ',' (')' | '**')) { RAISE_SYNTAX_ERROR("named arguments must follow bare *") }
    | '*' ',' TYPE_COMMENT { RAISE_SYNTAX_ERROR("bare * has associated type comment") }
invalid_lambda_star_etc:
    | '*' (':' | ',' (':' | '**')) { RAISE_SYNTAX_ERROR("named arguments must follow bare *") }
invalid_double_type_comments:
    | TYPE_COMMENT NEWLINE TYPE_COMMENT NEWLINE INDENT {
        RAISE_SYNTAX_ERROR("Cannot have two type comments on def") }
invalid_with_item:
    | expression 'as' a=expression &(',' | ')' | ':') {
        RAISE_SYNTAX_ERROR_INVALID_TARGET(STAR_TARGETS, a) }

invalid_for_target:
    | ASYNC? 'for' a=star_expressions {
        RAISE_SYNTAX_ERROR_INVALID_TARGET(FOR_TARGETS, a) }

invalid_group:
    | '(' a=starred_expression ')' {
        RAISE_SYNTAX_ERROR_KNOWN_LOCATION(a, "cannot use starred expression here") }
    | '(' a='**' expression ')' {
        RAISE_SYNTAX_ERROR_KNOWN_LOCATION(a, "cannot use double starred expression here") }
invalid_import_from_targets:
    | import_from_as_names ',' {
        RAISE_SYNTAX_ERROR("trailing comma not allowed without surrounding parentheses") }

invalid_with_stmt:
    | [ASYNC] 'with' ','.(expression ['as' star_target])+ &&':'
    | [ASYNC] 'with' '(' ','.(expressions ['as' star_target])+ ','? ')' &&':'

invalid_except_block:
    | 'except' a=expression ',' expressions ['as' NAME ] ':' {
        RAISE_SYNTAX_ERROR_KNOWN_LOCATION(a, "exception group must be parenthesized") }
    | 'except' expression ['as' NAME ] &&':'
    | 'except' &&':'

invalid_match_stmt:
    | "match" subject_expr !':' { CHECK_VERSION(void*, 10, "Pattern matching is", RAISE_SYNTAX_ERROR("expected ':'") ) }
invalid_case_block:
    | "case" patterns guard? !':' { RAISE_SYNTAX_ERROR("expected ':'") }
invalid_if_stmt:
    | 'if' named_expression NEWLINE { RAISE_SYNTAX_ERROR("expected ':'") }
invalid_elif_stmt:
    | 'elif' named_expression NEWLINE { RAISE_SYNTAX_ERROR("expected ':'") }
invalid_while_stmt:
    | 'while' named_expression NEWLINE { RAISE_SYNTAX_ERROR("expected ':'") }

invalid_double_starred_kvpairs:
    | ','.double_starred_kvpair+ ',' invalid_kvpair
    | expression ':' a='*' bitwise_or { RAISE_SYNTAX_ERROR_KNOWN_LOCATION(a, "cannot use a starred expression in a dictionary value") }
    | expression a=':' &('}'|',') { RAISE_SYNTAX_ERROR_KNOWN_LOCATION(a, "expression expected after dictionary key and ':'") }
invalid_kvpair:
    | a=expression !(':') {
        RAISE_ERROR_KNOWN_LOCATION(p, PyExc_SyntaxError, a->lineno, a->end_col_offset - 1, "':' expected after dictionary key") }
    | expression ':' a='*' bitwise_or { RAISE_SYNTAX_ERROR_KNOWN_LOCATION(a, "cannot use a starred expression in a dictionary value") }
    | expression a=':' {RAISE_SYNTAX_ERROR_KNOWN_LOCATION(a, "expression expected after dictionary key and ':'") }
